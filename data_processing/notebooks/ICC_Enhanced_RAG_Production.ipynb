{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50650fe1-170e-4a1f-8d88-8156c5adc224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ICC Enhanced RAG System - Production Deployment\n",
    "\n",
    "**Architecture:**\n",
    "- üîç **Enhanced Vector Search**: Dual-index retrieval with intelligent routing using `databricks-gte-large-en`\n",
    "- üß† **Advanced LLM**: `databricks-meta-llama-3-3-70b-instruct` for legal analysis\n",
    "- üöÄ **MLflow 3.0**: Production deployment and model management\n",
    "- ‚öñÔ∏è **Legal Expertise**: Specialized for ICC defense team research\n",
    "\n",
    "**Data Sources:**\n",
    "- **Past Judgments Index**: `past_judgement` (ICTY/ICC case law)\n",
    "- **Geneva Documentation Index**: `geneva_documentation` (IHL framework)\n",
    "- **Vector Search Endpoint**: `jgmt` (with databricks-gte-large-en embedding model)\n",
    "\n",
    "**Key Features:**\n",
    "- Intelligent routing based on legal topics\n",
    "- Enhanced retrieval with relevance boosting\n",
    "- Comprehensive legal analysis generation\n",
    "- Production-ready MLflow 3.0 deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5df41df7-fd82-45a7-acac-3a292089c0a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -qqqq mlflow>=3.1.1 langchain databricks-langchain pydantic databricks-agents unitycatalog-langchain[databricks] uv databricks-feature-engineering==0.12.1\n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c049d49f-52aa-4dc1-99aa-e4ff9e5c0085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced RAG dependencies loaded\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "import datetime\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.models.resources import (\n",
    "    DatabricksVectorSearchIndex,\n",
    "    DatabricksServingEndpoint\n",
    ")\n",
    "\n",
    "# Vector Search and LLM\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "print(\"‚úÖ Enhanced RAG dependencies loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb3e2c5a-878c-4c0c-9ba7-82f76513e522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Enhanced Configuration & Legal Topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0b1d049-3080-48c7-b6c6-dbb86d65b0ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced configuration loaded with legal topics\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Configuration\n",
    "VECTOR_SEARCH_ENDPOINT = \"jgmt\"\n",
    "PAST_JUDGMENTS_INDEX = \"icc_chatbot.search_model.past_judgement\"\n",
    "GENEVA_DOCUMENTATION_INDEX = \"icc_chatbot.search_model.geneva_documentation\"\n",
    "LLM_MODEL_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "\n",
    "# Search parameters\n",
    "DEFAULT_TOP_K = 10\n",
    "MAX_CONTEXT_LENGTH = 4000\n",
    "SIMILARITY_THRESHOLD = 0.7\n",
    "MAX_TOKENS = 2048\n",
    "TEMPERATURE = 0.1\n",
    "\n",
    "# Legal topics for intelligent routing\n",
    "LEGAL_TOPICS = {\n",
    "    \"judgment_priority\": [\n",
    "        \"overall control\", \"state\", \"protected persons\", \"active participation\", \"direct participation\",\n",
    "        \"combatant status\", \"combatant privilege\", \"civilian status\", \"duty to protect\",\n",
    "        \"organisation of armed groups\", \"principle of distinction\", \"indiscriminate attack\",\n",
    "        \"civilian population\", \"military objectives\", \"military objects\", \"rule of proportionality\",\n",
    "        \"principle of proportionality\", \"collateral damage\", \"military necessity\",\n",
    "        \"military imperative\", \"security of civilians\", \"imperative military reasons\",\n",
    "        \"conduct of hostilities\", \"means of warfare\", \"methods of warfare\",\n",
    "        \"attacks against protected objects\", \"religious buildings\", \"displacement\",\n",
    "        \"deportation\", \"coercion\", \"cruel treatment\", \"torture\", \"outrages against dignity\",\n",
    "        \"murder\", \"self-defense\", \"causal link\", \"checkpoints\", \"roadblocks\",\n",
    "        \"icty\", \"trial chamber\", \"appeals chamber\", \"judgment\", \"applied\", \"practice\"\n",
    "    ],\n",
    "    \"geneva_priority\": [\n",
    "        \"geneva convention\", \"international humanitarian law\", \"ihl\", \"protected persons\",\n",
    "        \"wounded and sick\", \"prisoners of war\", \"civilians\", \"medical personnel\",\n",
    "        \"religious personnel\", \"cultural property\", \"distinctive emblems\", \"red cross\",\n",
    "        \"red crescent\", \"additional protocol\", \"grave breaches\", \"serious violations\",\n",
    "        \"customary international law\", \"treaty law\", \"convention\", \"protocol\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Enhanced configuration loaded with legal topics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12d6f058-9a6e-4f44-8842-11335f9ddcf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced data structures defined\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class SearchResult:\n",
    "    \"\"\"Enhanced search result with comprehensive metadata\"\"\"\n",
    "    content: str\n",
    "    summary: str\n",
    "    source: str\n",
    "    metadata: Dict[str, Any]\n",
    "    score: float\n",
    "    source_type: str  # 'judgment' or 'geneva'\n",
    "    page_number: Optional[int] = None\n",
    "    article: Optional[str] = None\n",
    "    section: Optional[str] = None\n",
    "    document_type: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class RetrievalContext:\n",
    "    \"\"\"Enhanced retrieval context with routing information\"\"\"\n",
    "    question: str\n",
    "    routing_decision: str\n",
    "    judgment_results: \"List[SearchResult]\"\n",
    "    geneva_results: \"List[SearchResult]\"\n",
    "    all_results: \"List[SearchResult]\"\n",
    "    total_sources: int\n",
    "    processing_time: float\n",
    "    \n",
    "@dataclass\n",
    "class LegalAnalysis:\n",
    "    \"\"\"Structured legal analysis result\"\"\"\n",
    "    question: str\n",
    "    analysis: str\n",
    "    sources_used: \"List[SearchResult]\"\n",
    "    key_findings: List[str]\n",
    "    citations: List[str]\n",
    "    confidence_score: float\n",
    "    processing_time: float\n",
    "\n",
    "print(\"‚úÖ Enhanced data structures defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "980d7f7a-96a3-4159-8e29-bf8454a432be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Enhanced Data Structures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e292a2c-72c6-4528-81a9-e6f5d34ae142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced ICC RAG System core defined\n"
     ]
    }
   ],
   "source": [
    "class EnhancedICCRAGSystem:\n",
    "    \"\"\"Enhanced ICC RAG system with intelligent routing and legal expertise.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize clients\n",
    "        self.vsc = VectorSearchClient()\n",
    "        self.w = WorkspaceClient()\n",
    "        self.llm = ChatDatabricks(\n",
    "            target_uri=\"databricks\",\n",
    "            endpoint=LLM_MODEL_ENDPOINT,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS\n",
    "        )\n",
    "        \n",
    "        # Conversation memory\n",
    "        self.conversations = {}\n",
    "        \n",
    "        # Legal terminology for query enhancement\n",
    "        self.legal_expansions = {\n",
    "            \"war crimes\": [\"war crime\", \"violations of laws of war\", \"grave breaches\"],\n",
    "            \"crimes against humanity\": [\"crime against humanity\", \"systematic attack\", \"persecution\"],\n",
    "            \"persecution\": [\"persecute\", \"persecuted\", \"discriminatory acts\", \"discriminatory intent\"],\n",
    "            \"murder\": [\"kill\", \"killing\", \"unlawful killing\", \"wilful killing\"],\n",
    "            \"active participation\": [\"direct participation\", \"hostilities\", \"combatant status\"],\n",
    "            \"civilian status\": [\"protected person\", \"civilian population\", \"non-combatant\"],\n",
    "            \"combatant status\": [\"combatant privilege\", \"armed forces\", \"military objective\"]\n",
    "        }\n",
    "    \n",
    "    def determine_routing_priority(self, question: str) -> str:\n",
    "        \"\"\"Determine which index to prioritize based on question content.\"\"\"\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        # Count matches for each topic category\n",
    "        judgment_matches = sum(1 for topic in LEGAL_TOPICS[\"judgment_priority\"] \n",
    "                              if topic in question_lower)\n",
    "        geneva_matches = sum(1 for topic in LEGAL_TOPICS[\"geneva_priority\"] \n",
    "                            if topic in question_lower)\n",
    "        \n",
    "        # Determine routing based on matches and question patterns\n",
    "        if \"icty\" in question_lower or \"trial\" in question_lower or \"appeal\" in question_lower:\n",
    "            return \"judgment\"\n",
    "        elif \"geneva\" in question_lower or \"convention\" in question_lower:\n",
    "            return \"geneva\"\n",
    "        elif judgment_matches > geneva_matches and judgment_matches > 0:\n",
    "            return \"judgment\"\n",
    "        elif geneva_matches > judgment_matches and geneva_matches > 0:\n",
    "            return \"geneva\"\n",
    "        elif judgment_matches > 0 and geneva_matches > 0:\n",
    "            return \"both\"\n",
    "        else:\n",
    "            return \"both\"  # Default to both if no clear indicators\n",
    "    \n",
    "    def enhance_query(self, query: str) -> str:\n",
    "        \"\"\"Enhance query for better retrieval using legal terminology.\"\"\"\n",
    "        enhanced = query.lower()\n",
    "        \n",
    "        # Add legal term expansions\n",
    "        for term, expansions in self.legal_expansions.items():\n",
    "            if term in enhanced:\n",
    "                enhanced += f\" {' '.join(expansions[:2])}\"\n",
    "        \n",
    "        return enhanced\n",
    "\n",
    "print(\"‚úÖ Enhanced ICC RAG System core defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc2d82c-3c70-4e06-8506-f48a07fbb34d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Core methods added to Enhanced ICC RAG System\n"
     ]
    }
   ],
   "source": [
    "# Add missing core methods to the EnhancedICCRAGSystem class\n",
    "def add_core_methods_to_rag_system():\n",
    "    \"\"\"Add the missing retrieve_context and generate_legal_analysis methods.\"\"\"\n",
    "    \n",
    "    def retrieve_context(self, query: str, top_k: int = DEFAULT_TOP_K) -> \"RetrievalContext\":\n",
    "        \"\"\"Retrieve context from both indices with intelligent routing.\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Determine routing priority\n",
    "        routing_decision = self.determine_routing_priority(query)\n",
    "        \n",
    "        # Enhance query for better retrieval\n",
    "        enhanced_query = self.enhance_query(query)\n",
    "        \n",
    "        # Initialize results\n",
    "        judgment_results = []\n",
    "        geneva_results = []\n",
    "        \n",
    "        # Search based on routing decision\n",
    "        if routing_decision in [\"judgment\", \"both\"]:\n",
    "            judgment_results = self.search_past_judgments(enhanced_query, top_k)\n",
    "        \n",
    "        if routing_decision in [\"geneva\", \"both\"]:\n",
    "            geneva_results = self.search_geneva_documentation(enhanced_query, top_k)\n",
    "        \n",
    "        # Combine and sort all results by score\n",
    "        all_results = judgment_results + geneva_results\n",
    "        all_results.sort(key=lambda x: x.score, reverse=True)\n",
    "        \n",
    "        # Limit to top results\n",
    "        all_results = all_results[:top_k]\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        return RetrievalContext(\n",
    "            question=query,\n",
    "            routing_decision=routing_decision,\n",
    "            judgment_results=judgment_results,\n",
    "            geneva_results=geneva_results,\n",
    "            all_results=all_results,\n",
    "            total_sources=len(all_results),\n",
    "            processing_time=processing_time\n",
    "        )\n",
    "    \n",
    "    def generate_legal_analysis(self, question: str, context: \"RetrievalContext\", conversation_id: str = None) -> \"LegalAnalysis\":\n",
    "        \"\"\"Generate comprehensive legal analysis using the retrieved context.\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Prepare context for LLM\n",
    "        context_text = self._prepare_context_for_llm(context.all_results)\n",
    "        \n",
    "        # Create conversation memory if needed\n",
    "        if conversation_id and conversation_id not in self.conversations:\n",
    "            self.conversations[conversation_id] = ConversationBufferWindowMemory(\n",
    "                k=5,  # Keep last 5 exchanges\n",
    "                return_messages=True\n",
    "            )\n",
    "        \n",
    "        # Build prompt\n",
    "        system_prompt = \"\"\"You are an expert legal researcher specializing in International Criminal Law and International Humanitarian Law. \n",
    "        You have access to comprehensive databases of ICTY/ICC judgments and Geneva Convention documentation.\n",
    "        \n",
    "        Your task is to provide thorough, accurate legal analysis based on the retrieved context. Always:\n",
    "        1. Cite specific sources and page numbers when available\n",
    "        2. Identify key legal principles and precedents\n",
    "        3. Highlight relevant case law and treaty provisions\n",
    "        4. Provide clear, structured analysis\n",
    "        5. Note any limitations or gaps in the available information\n",
    "        \n",
    "        Be precise, professional, and comprehensive in your analysis.\"\"\"\n",
    "        \n",
    "        human_prompt = f\"\"\"Legal Research Question: {question}\n",
    "\n",
    "Retrieved Context:\n",
    "{context_text}\n",
    "\n",
    "Please provide a comprehensive legal analysis addressing the question above. Include:\n",
    "1. Key findings from the retrieved sources\n",
    "2. Relevant legal principles and precedents\n",
    "3. Specific citations to judgments, articles, or sections\n",
    "4. Analysis of the legal framework\n",
    "5. Any limitations or areas requiring further research\n",
    "\n",
    "Format your response with clear headings and bullet points for readability.\"\"\"\n",
    "        \n",
    "        # Generate analysis\n",
    "        try:\n",
    "            if conversation_id and conversation_id in self.conversations:\n",
    "                # Use conversation memory\n",
    "                memory = self.conversations[conversation_id]\n",
    "                messages = memory.chat_memory.messages\n",
    "                messages.extend([\n",
    "                    SystemMessage(content=system_prompt),\n",
    "                    HumanMessage(content=human_prompt)\n",
    "                ])\n",
    "                response = self.llm(messages)\n",
    "                memory.chat_memory.add_message(HumanMessage(content=question))\n",
    "                memory.chat_memory.add_message(response)\n",
    "            else:\n",
    "                # Direct generation\n",
    "                messages = [\n",
    "                    SystemMessage(content=system_prompt),\n",
    "                    HumanMessage(content=human_prompt)\n",
    "                ]\n",
    "                response = self.llm(messages)\n",
    "            \n",
    "            analysis_text = response.content\n",
    "            \n",
    "            # Extract key findings and citations\n",
    "            key_findings = self._extract_key_findings(analysis_text)\n",
    "            citations = self._extract_citations(analysis_text, context.all_results)\n",
    "            \n",
    "            # Calculate confidence score based on source quality and quantity\n",
    "            confidence_score = self._calculate_confidence_score(context.all_results, len(key_findings))\n",
    "            \n",
    "        except Exception as e:\n",
    "            analysis_text = f\"Error generating analysis: {str(e)}\"\n",
    "            key_findings = []\n",
    "            citations = []\n",
    "            confidence_score = 0.0\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        return LegalAnalysis(\n",
    "            question=question,\n",
    "            analysis=analysis_text,\n",
    "            sources_used=context.all_results,\n",
    "            key_findings=key_findings,\n",
    "            citations=citations,\n",
    "            confidence_score=confidence_score,\n",
    "            processing_time=processing_time\n",
    "        )\n",
    "    \n",
    "    def _prepare_context_for_llm(self, results: \"List[SearchResult]\") -> str:\n",
    "        \"\"\"Prepare retrieved results for LLM consumption.\"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            context_part = f\"Source {i} ({result.source_type.upper()}):\\n\"\n",
    "            context_part += f\"Document: {result.source}\\n\"\n",
    "            if result.section:\n",
    "                context_part += f\"Section: {result.section}\\n\"\n",
    "            if result.page_number:\n",
    "                context_part += f\"Page: {result.page_number}\\n\"\n",
    "            context_part += f\"Relevance Score: {result.score:.3f}\\n\"\n",
    "            context_part += f\"Content: {result.content[:1000]}...\\n\"  # Limit content length\n",
    "            context_parts.append(context_part)\n",
    "        \n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    def _extract_key_findings(self, analysis_text: str) -> List[str]:\n",
    "        \"\"\"Extract key findings from the analysis text.\"\"\"\n",
    "        # Simple extraction based on common patterns\n",
    "        findings = []\n",
    "        lines = analysis_text.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if (line.startswith('‚Ä¢') or line.startswith('-') or \n",
    "                line.startswith('1.') or line.startswith('2.') or\n",
    "                'finding' in line.lower() or 'principle' in line.lower()):\n",
    "                findings.append(line)\n",
    "        \n",
    "        return findings[:10]  # Limit to top 10 findings\n",
    "    \n",
    "    def _extract_citations(self, analysis_text: str, sources: \"List[SearchResult]\") -> List[str]:\n",
    "        \"\"\"Extract citations from the analysis text.\"\"\"\n",
    "        citations = []\n",
    "        \n",
    "        # Extract source references\n",
    "        for source in sources:\n",
    "            if source.page_number:\n",
    "                citations.append(f\"{source.source}, Page {source.page_number}\")\n",
    "            elif source.article:\n",
    "                citations.append(f\"{source.source}, Article {source.article}\")\n",
    "            else:\n",
    "                citations.append(source.source)\n",
    "        \n",
    "        return citations[:15]  # Limit to top 15 citations\n",
    "    \n",
    "    def _calculate_confidence_score(self, sources: \"List[SearchResult]\", findings_count: int) -> float:\n",
    "        \"\"\"Calculate confidence score based on source quality and analysis depth.\"\"\"\n",
    "        if not sources:\n",
    "            return 0.0\n",
    "        \n",
    "        # Base score from source quality\n",
    "        avg_score = sum(s.score for s in sources) / len(sources)\n",
    "        \n",
    "        # Bonus for number of sources\n",
    "        source_bonus = min(len(sources) / 10.0, 0.2)\n",
    "        \n",
    "        # Bonus for findings\n",
    "        findings_bonus = min(findings_count / 5.0, 0.2)\n",
    "        \n",
    "        # Combine scores\n",
    "        confidence = min(avg_score + source_bonus + findings_bonus, 1.0)\n",
    "        \n",
    "        return round(confidence, 3)\n",
    "    \n",
    "    # Add methods to the class\n",
    "    EnhancedICCRAGSystem.retrieve_context = retrieve_context\n",
    "    EnhancedICCRAGSystem.generate_legal_analysis = generate_legal_analysis\n",
    "    EnhancedICCRAGSystem._prepare_context_for_llm = _prepare_context_for_llm\n",
    "    EnhancedICCRAGSystem._extract_key_findings = _extract_key_findings\n",
    "    EnhancedICCRAGSystem._extract_citations = _extract_citations\n",
    "    EnhancedICCRAGSystem._calculate_confidence_score = _calculate_confidence_score\n",
    "    \n",
    "    print(\"‚úÖ Core methods added to Enhanced ICC RAG System\")\n",
    "\n",
    "# Execute the function to add methods\n",
    "add_core_methods_to_rag_system()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb59e767-6ec1-426f-8442-92ed68c60c2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Enhanced RAG System Core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39ea115d-ee53-42ce-9690-5b3445af118c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Search methods added to Enhanced ICC RAG System\n"
     ]
    }
   ],
   "source": [
    "# Add search methods to the EnhancedICCRAGSystem class\n",
    "def add_search_methods_to_rag_system():\n",
    "    \"\"\"Add search methods to the RAG system class.\"\"\"\n",
    "    \n",
    "    def search_past_judgments(self, query: str, top_k: int = DEFAULT_TOP_K) -> \"List[SearchResult]\":\n",
    "        \"\"\"Search past judgments using vector search with enhanced metadata.\"\"\"\n",
    "        try:\n",
    "            # Use columns parameter as it's required by the API\n",
    "            results = self.vsc.get_index(VECTOR_SEARCH_ENDPOINT, PAST_JUDGMENTS_INDEX).similarity_search(\n",
    "                query_text=query,\n",
    "                columns=[\"text\", \"summary\", \"doc_id\", \"section_type\", \"pages\"],\n",
    "                num_results=top_k\n",
    "            )\n",
    "            \n",
    "\n",
    "            search_results = []\n",
    "            for i, result in enumerate(results):\n",
    "                try:\n",
    "                    # Handle different result formats\n",
    "                    if isinstance(result, str):\n",
    "                        # If result is a string, create a basic SearchResult\n",
    "                        search_results.append(SearchResult(\n",
    "                            content=result,\n",
    "                            summary=\"\",\n",
    "                            source=f\"Document_{i+1}\",\n",
    "                            metadata={\"score\": 0.5},\n",
    "                            score=0.5,\n",
    "                            source_type=\"judgment\",\n",
    "                            page_number=None,\n",
    "                            section=\"\"\n",
    "                        ))\n",
    "                    elif isinstance(result, dict):\n",
    "                        # If result is a dictionary, extract fields safely\n",
    "                        pages = result.get(\"pages\", [])\n",
    "                        page_number = pages[0] if pages and len(pages) > 0 else None\n",
    "                        \n",
    "                        search_results.append(SearchResult(\n",
    "                            content=result.get(\"text\", \"\"),\n",
    "                            summary=result.get(\"summary\", \"\"),\n",
    "                            source=result.get(\"doc_id\", f\"Document_{i+1}\"),\n",
    "                            metadata={\n",
    "                                \"section_type\": result.get(\"section_type\", \"\"),\n",
    "                                \"score\": result.get(\"score\", 0.0)\n",
    "                            },\n",
    "                            score=result.get(\"score\", 0.0),\n",
    "                            source_type=\"judgment\",\n",
    "                            page_number=page_number,\n",
    "                            section=result.get(\"section_type\", \"\")\n",
    "                        ))\n",
    "                    else:\n",
    "                        # Handle other types (e.g., custom objects)\n",
    "                        search_results.append(SearchResult(\n",
    "                            content=str(result),\n",
    "                            summary=\"\",\n",
    "                            source=f\"Document_{i+1}\",\n",
    "                            metadata={\"score\": 0.5},\n",
    "                            score=0.5,\n",
    "                            source_type=\"judgment\",\n",
    "                            page_number=None,\n",
    "                            section=\"\"\n",
    "                        ))\n",
    "                except Exception as item_error:\n",
    "                    print(f\"Error processing result {i}: {item_error}\")\n",
    "                    # Create a fallback result\n",
    "                    search_results.append(SearchResult(\n",
    "                        content=str(result) if result else \"\",\n",
    "                        summary=\"\",\n",
    "                        source=f\"Document_{i+1}\",\n",
    "                        metadata={\"score\": 0.0},\n",
    "                        score=0.0,\n",
    "                        source_type=\"judgment\",\n",
    "                        page_number=None,\n",
    "                        section=\"\"\n",
    "                    ))\n",
    "            \n",
    "            return search_results\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching past judgments: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_geneva_documentation(self, query: str, top_k: int = DEFAULT_TOP_K) -> \"List[SearchResult]\":\n",
    "        \"\"\"Search Geneva Convention documentation using vector search.\"\"\"\n",
    "        try:\n",
    "            # Use columns parameter as it's required by the API\n",
    "            results = self.vsc.get_index(VECTOR_SEARCH_ENDPOINT, GENEVA_DOCUMENTATION_INDEX).similarity_search(\n",
    "                query_text=query,\n",
    "                columns=[\"text\", \"summary\", \"doc_name\", \"section_type\", \"pages\"],\n",
    "                num_results=top_k\n",
    "            )\n",
    "            \n",
    "\n",
    "            search_results = []\n",
    "            for i, result in enumerate(results):\n",
    "                try:\n",
    "                    # Handle different result formats\n",
    "                    if isinstance(result, str):\n",
    "                        # If result is a string, create a basic SearchResult\n",
    "                        search_results.append(SearchResult(\n",
    "                            content=result,\n",
    "                            summary=\"\",\n",
    "                            source=f\"Geneva_Document_{i+1}\",\n",
    "                            metadata={\"score\": 0.5},\n",
    "                            score=0.5,\n",
    "                            source_type=\"geneva\",\n",
    "                            page_number=None,\n",
    "                            section=\"\"\n",
    "                        ))\n",
    "                    elif isinstance(result, dict):\n",
    "                        # If result is a dictionary, extract fields safely\n",
    "                        pages = result.get(\"pages\", [])\n",
    "                        page_number = pages[0] if pages and len(pages) > 0 else None\n",
    "                        \n",
    "                        search_results.append(SearchResult(\n",
    "                            content=result.get(\"text\", \"\"),\n",
    "                            summary=result.get(\"summary\", \"\"),\n",
    "                            source=result.get(\"doc_name\", f\"Geneva_Document_{i+1}\"),\n",
    "                            metadata={\n",
    "                                \"section_type\": result.get(\"section_type\", \"\"),\n",
    "                                \"score\": result.get(\"score\", 0.0)\n",
    "                            },\n",
    "                            score=result.get(\"score\", 0.0),\n",
    "                            source_type=\"geneva\",\n",
    "                            page_number=page_number,\n",
    "                            section=result.get(\"section_type\", \"\")\n",
    "                        ))\n",
    "                    else:\n",
    "                        # Handle other types (e.g., custom objects)\n",
    "                        search_results.append(SearchResult(\n",
    "                            content=str(result),\n",
    "                            summary=\"\",\n",
    "                            source=f\"Geneva_Document_{i+1}\",\n",
    "                            metadata={\"score\": 0.5},\n",
    "                            score=0.5,\n",
    "                            source_type=\"geneva\",\n",
    "                            page_number=None,\n",
    "                            section=\"\"\n",
    "                        ))\n",
    "                except Exception as item_error:\n",
    "                    print(f\"Error processing Geneva result {i}: {item_error}\")\n",
    "                    # Create a fallback result\n",
    "                    search_results.append(SearchResult(\n",
    "                        content=str(result) if result else \"\",\n",
    "                        summary=\"\",\n",
    "                        source=f\"Geneva_Document_{i+1}\",\n",
    "                        metadata={\"score\": 0.0},\n",
    "                        score=0.0,\n",
    "                        source_type=\"geneva\",\n",
    "                        page_number=None,\n",
    "                        section=\"\"\n",
    "                    ))\n",
    "            \n",
    "            return search_results\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching Geneva documentation: {e}\")\n",
    "            return []\n",
    "    \n",
    "    # Add methods to the class\n",
    "    EnhancedICCRAGSystem.search_past_judgments = search_past_judgments\n",
    "    EnhancedICCRAGSystem.search_geneva_documentation = search_geneva_documentation\n",
    "    \n",
    "    print(\"‚úÖ Search methods added to Enhanced ICC RAG System\")\n",
    "\n",
    "# Execute the function to add methods\n",
    "add_search_methods_to_rag_system()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bd372f2-2fdf-4481-8b2d-4951ad31d967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test Legal Research Questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fixed search methods\n",
    "def test_fixed_search_methods():\n",
    "    \"\"\"Test the fixed search methods to ensure they work correctly.\"\"\"\n",
    "    \n",
    "    print(\"üîß TESTING FIXED SEARCH METHODS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize the system\n",
    "    rag_system = EnhancedICCRAGSystem()\n",
    "    \n",
    "    # Test simple queries\n",
    "    test_queries = [\n",
    "        \"What is active participation in hostilities?\",\n",
    "        \"Geneva Convention protected persons\",\n",
    "        \"ICTY trial judgment civilian status\"\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\nüîç Test Query {i}: {query}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # Test past judgments search\n",
    "            print(\"Testing past judgments search...\")\n",
    "            judgment_results = rag_system.search_past_judgments(query, top_k=3)\n",
    "            print(f\"‚úÖ Past judgments: {len(judgment_results)} results\")\n",
    "            \n",
    "            # Test Geneva documentation search\n",
    "            print(\"Testing Geneva documentation search...\")\n",
    "            geneva_results = rag_system.search_geneva_documentation(query, top_k=3)\n",
    "            print(f\"‚úÖ Geneva documentation: {len(geneva_results)} results\")\n",
    "            \n",
    "            # Test full context retrieval\n",
    "            print(\"Testing full context retrieval...\")\n",
    "            context = rag_system.retrieve_context(query, top_k=5)\n",
    "            print(f\"‚úÖ Context retrieval: {context.total_sources} total sources\")\n",
    "            print(f\"   Routing decision: {context.routing_decision}\")\n",
    "            print(f\"   Processing time: {context.processing_time:.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in test {i}: {e}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Search method testing completed!\")\n",
    "    return True\n",
    "\n",
    "# Run the test\n",
    "test_fixed_search_methods()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved search methods with proper column handling\n",
    "def add_improved_search_methods():\n",
    "    \"\"\"Add improved search methods that can handle columns parameter properly.\"\"\"\n",
    "    \n",
    "    def search_past_judgments_improved(self, query: str, top_k: int = DEFAULT_TOP_K) -> \"List[SearchResult]\":\n",
    "        \"\"\"Improved search past judgments with better column handling.\"\"\"\n",
    "        try:\n",
    "            # Use columns parameter as it's required by the API\n",
    "            results = self.vsc.get_index(VECTOR_SEARCH_ENDPOINT, PAST_JUDGMENTS_INDEX).similarity_search(\n",
    "                query_text=query,\n",
    "                columns=[\"text\", \"summary\", \"doc_id\", \"section_type\", \"pages\"],\n",
    "                num_results=top_k\n",
    "            )\n",
    "            \n",
    "            search_results = []\n",
    "            for i, result in enumerate(results):\n",
    "                try:\n",
    "                    if isinstance(result, dict):\n",
    "                        # Extract fields safely\n",
    "                        pages = result.get(\"pages\", [])\n",
    "                        page_number = pages[0] if pages and len(pages) > 0 else None\n",
    "                        \n",
    "                        search_results.append(SearchResult(\n",
    "                            content=result.get(\"text\", \"\"),\n",
    "                            summary=result.get(\"summary\", \"\"),\n",
    "                            source=result.get(\"doc_id\", f\"Document_{i+1}\"),\n",
    "                            metadata={\n",
    "                                \"section_type\": result.get(\"section_type\", \"\"),\n",
    "                                \"score\": result.get(\"score\", 0.0)\n",
    "                            },\n",
    "                            score=result.get(\"score\", 0.0),\n",
    "                            source_type=\"judgment\",\n",
    "                            page_number=page_number,\n",
    "                            section=result.get(\"section_type\", \"\")\n",
    "                        ))\n",
    "                    else:\n",
    "                        # Handle non-dict results\n",
    "                        search_results.append(SearchResult(\n",
    "                            content=str(result),\n",
    "                            summary=\"\",\n",
    "                            source=f\"Document_{i+1}\",\n",
    "                            metadata={\"score\": 0.5},\n",
    "                            score=0.5,\n",
    "                            source_type=\"judgment\",\n",
    "                            page_number=None,\n",
    "                            section=\"\"\n",
    "                        ))\n",
    "                except Exception as item_error:\n",
    "                    print(f\"Error processing result {i}: {item_error}\")\n",
    "                    search_results.append(SearchResult(\n",
    "                        content=str(result) if result else \"\",\n",
    "                        summary=\"\",\n",
    "                        source=f\"Document_{i+1}\",\n",
    "                        metadata={\"score\": 0.0},\n",
    "                        score=0.0,\n",
    "                        source_type=\"judgment\",\n",
    "                        page_number=None,\n",
    "                        section=\"\"\n",
    "                    ))\n",
    "            \n",
    "            return search_results\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching past judgments: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_geneva_documentation_improved(self, query: str, top_k: int = DEFAULT_TOP_K) -> \"List[SearchResult]\":\n",
    "        \"\"\"Improved search Geneva documentation with better column handling.\"\"\"\n",
    "        try:\n",
    "            # Use columns parameter as it's required by the API\n",
    "            results = self.vsc.get_index(VECTOR_SEARCH_ENDPOINT, GENEVA_DOCUMENTATION_INDEX).similarity_search(\n",
    "                query_text=query,\n",
    "                columns=[\"text\", \"summary\", \"doc_name\", \"section_type\", \"pages\"],\n",
    "                num_results=top_k\n",
    "            )\n",
    "            \n",
    "            search_results = []\n",
    "            for i, result in enumerate(results):\n",
    "                try:\n",
    "                    if isinstance(result, dict):\n",
    "                        # Extract fields safely\n",
    "                        pages = result.get(\"pages\", [])\n",
    "                        page_number = pages[0] if pages and len(pages) > 0 else None\n",
    "                        \n",
    "                        search_results.append(SearchResult(\n",
    "                            content=result.get(\"text\", \"\"),\n",
    "                            summary=result.get(\"summary\", \"\"),\n",
    "                            source=result.get(\"doc_name\", f\"Geneva_Document_{i+1}\"),\n",
    "                            metadata={\n",
    "                                \"section_type\": result.get(\"section_type\", \"\"),\n",
    "                                \"score\": result.get(\"score\", 0.0)\n",
    "                            },\n",
    "                            score=result.get(\"score\", 0.0),\n",
    "                            source_type=\"geneva\",\n",
    "                            page_number=page_number,\n",
    "                            section=result.get(\"section_type\", \"\")\n",
    "                        ))\n",
    "                    else:\n",
    "                        # Handle non-dict results\n",
    "                        search_results.append(SearchResult(\n",
    "                            content=str(result),\n",
    "                            summary=\"\",\n",
    "                            source=f\"Geneva_Document_{i+1}\",\n",
    "                            metadata={\"score\": 0.5},\n",
    "                            score=0.5,\n",
    "                            source_type=\"geneva\",\n",
    "                            page_number=None,\n",
    "                            section=\"\"\n",
    "                        ))\n",
    "                except Exception as item_error:\n",
    "                    print(f\"Error processing Geneva result {i}: {item_error}\")\n",
    "                    search_results.append(SearchResult(\n",
    "                        content=str(result) if result else \"\",\n",
    "                        summary=\"\",\n",
    "                        source=f\"Geneva_Document_{i+1}\",\n",
    "                        metadata={\"score\": 0.0},\n",
    "                        score=0.0,\n",
    "                        source_type=\"geneva\",\n",
    "                        page_number=None,\n",
    "                        section=\"\"\n",
    "                    ))\n",
    "            \n",
    "            return search_results\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching Geneva documentation: {e}\")\n",
    "            return []\n",
    "    \n",
    "    # Add improved methods to the class\n",
    "    EnhancedICCRAGSystem.search_past_judgments_improved = search_past_judgments_improved\n",
    "    EnhancedICCRAGSystem.search_geneva_documentation_improved = search_geneva_documentation_improved\n",
    "    \n",
    "    print(\"‚úÖ Improved search methods added to Enhanced ICC RAG System\")\n",
    "\n",
    "# Execute the function to add improved methods\n",
    "add_improved_search_methods()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to verify the columns parameter fix\n",
    "def quick_test_columns_fix():\n",
    "    \"\"\"Quick test to verify the columns parameter fix works.\"\"\"\n",
    "    \n",
    "    print(\"üîß QUICK TEST - COLUMNS PARAMETER FIX\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize the system\n",
    "    rag_system = EnhancedICCRAGSystem()\n",
    "    \n",
    "    # Test a simple query\n",
    "    test_query = \"active participation in hostilities\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"Testing query: '{test_query}'\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Test past judgments search\n",
    "        print(\"Testing past judgments search...\")\n",
    "        judgment_results = rag_system.search_past_judgments(test_query, top_k=2)\n",
    "        print(f\"‚úÖ Past judgments: {len(judgment_results)} results\")\n",
    "        \n",
    "        if judgment_results:\n",
    "            print(f\"   First result source: {judgment_results[0].source}\")\n",
    "            print(f\"   First result score: {judgment_results[0].score}\")\n",
    "        \n",
    "        # Test Geneva documentation search\n",
    "        print(\"Testing Geneva documentation search...\")\n",
    "        geneva_results = rag_system.search_geneva_documentation(test_query, top_k=2)\n",
    "        print(f\"‚úÖ Geneva documentation: {len(geneva_results)} results\")\n",
    "        \n",
    "        if geneva_results:\n",
    "            print(f\"   First result source: {geneva_results[0].source}\")\n",
    "            print(f\"   First result score: {geneva_results[0].score}\")\n",
    "        \n",
    "        print(f\"\\nüéâ All tests passed! The columns parameter fix is working.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during test: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the quick test\n",
    "quick_test_columns_fix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "291ade0a-6790-4659-87da-86a3f30a86e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "üß™ TESTING ENHANCED ICC RAG SYSTEM\n",
      "================================================================================\n",
      "\n",
      "################################################################################\n",
      "LEGAL RESEARCH QUESTION 1\n",
      "################################################################################\n",
      "Question: Can you please go through all the ICTY trial judgments and appeal judgments and identify where the chamber discusses the status of an individual durin...\n",
      "Expected routing: judgment\n",
      "Key topics: active participation, direct participation, civilian status, combatant status, ICTY, trial judgments, appeal judgments\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "Error searching past judgments: 'str' object has no attribute 'get'\n",
      "\n",
      "üìä ROUTING ANALYSIS:\n",
      "Expected: judgment\n",
      "Actual: judgment\n",
      "Sources found: 0\n",
      "Processing time: 0.62s\n",
      "\n",
      "‚öñÔ∏è LEGAL ANALYSIS:\n",
      "Confidence score: 0.000\n",
      "Key findings: 6\n",
      "Citations: 0\n",
      "Analysis length: 4577 characters\n",
      "\n",
      "üìù ANALYSIS PREVIEW:\n",
      "**Introduction**\n",
      "The International Criminal Tribunal for the former Yugoslavia (ICTY) has issued numerous trial and appeal judgments that address the status of individuals during conflict. This analysis aims to identify relevant paragraphs in ICTY judgments that discuss the active or direct participation of individuals, as well as their civilian or combatant status.\n",
      "\n",
      "**Key Findings**\n",
      "After reviewing the ICTY trial and appeal judgments, the following key findings were identified:\n",
      "\n",
      "* The ICTY has ...\n",
      "\n",
      "üîç KEY FINDINGS:\n",
      "1. **Key Findings**\n",
      "2. After reviewing the ICTY trial and appeal judgments, the following key findings were identified:\n",
      "3. * The ICTY has consistently applied the principles of international humanitarian law (IHL) to determine the status of individuals during conflict.\n",
      "\n",
      "üìö CITATIONS:\n",
      "\n",
      "################################################################################\n",
      "\n",
      "\n",
      "################################################################################\n",
      "LEGAL RESEARCH QUESTION 2\n",
      "################################################################################\n",
      "Question: Can you please go through all the ICTY trial judgments and appeal judgments and identify which factors the Trial or Appeals Chamber relied on in order...\n",
      "Expected routing: judgment\n",
      "Key topics: factors, assessment, actively participating, directly participating, hostilities, Trial Chamber, Appeals Chamber, citations\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "Error searching past judgments: 'str' object has no attribute 'get'\n",
      "\n",
      "üìä ROUTING ANALYSIS:\n",
      "Expected: judgment\n",
      "Actual: judgment\n",
      "Sources found: 0\n",
      "Processing time: 0.59s\n",
      "\n",
      "‚öñÔ∏è LEGAL ANALYSIS:\n",
      "Confidence score: 0.000\n",
      "Key findings: 7\n",
      "Citations: 0\n",
      "Analysis length: 5888 characters\n",
      "\n",
      "üìù ANALYSIS PREVIEW:\n",
      "**Introduction to Active or Direct Participation in Hostilities**\n",
      "=================================================================\n",
      "\n",
      "The concept of active or direct participation in hostilities is crucial in International Humanitarian Law (IHL), particularly in distinguishing between combatants and civilians. The International Criminal Tribunal for the former Yugoslavia (ICTY) has played a significant role in interpreting and applying this concept through its judgments. This analysis aims to ide...\n",
      "\n",
      "üîç KEY FINDINGS:\n",
      "1. **Key Findings from ICTY Judgments**\n",
      "2. ---------------------------------\n",
      "3. **Relevant Legal Principles and Precedents**\n",
      "\n",
      "üìö CITATIONS:\n",
      "\n",
      "################################################################################\n",
      "\n",
      "\n",
      "################################################################################\n",
      "LEGAL RESEARCH QUESTION 3\n",
      "################################################################################\n",
      "Question: Can you please search through all the ICTY trial judgments and appeal judgments and identify relevant paragraphs which would support the proposition t...\n",
      "Expected routing: judgment\n",
      "Key topics: enemy forces, armed, protected status, subjective assessment, objective assessment, lost status\n",
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True.\n",
      "Error searching past judgments: 'str' object has no attribute 'get'\n"
     ]
    }
   ],
   "source": [
    "# Test the Enhanced RAG System with complex legal research questions\n",
    "def test_enhanced_rag_system():\n",
    "    \"\"\"Test the enhanced RAG system with the provided legal research questions.\"\"\"\n",
    "    \n",
    "    # Initialize the system\n",
    "    rag_system = EnhancedICCRAGSystem()\n",
    "    \n",
    "    # Complex legal research queries\n",
    "    test_questions = [\n",
    "        {\n",
    "            \"question\": \"Can you please go through all the ICTY trial judgments and appeal judgments and identify where the chamber discusses the status of an individual during the conflict. In particular, please identify all relevant paragraphs where the chamber refers to the active or direct participation of the individual or where the chamber discusses the civilian status or combatant status of an individual. Please provide the direct paragraph in full.\",\n",
    "            \"expected_routing\": \"judgment\",\n",
    "            \"key_topics\": [\"active participation\", \"direct participation\", \"civilian status\", \"combatant status\", \"ICTY\", \"trial judgments\", \"appeal judgments\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Can you please go through all the ICTY trial judgments and appeal judgments and identify which factors the Trial or Appeals Chamber relied on in order to assess whether an individual is actively or directly participating in hostilities at a particular point? Please provide the full paragraph and citations\",\n",
    "            \"expected_routing\": \"judgment\", \n",
    "            \"key_topics\": [\"factors\", \"assessment\", \"actively participating\", \"directly participating\", \"hostilities\", \"Trial Chamber\", \"Appeals Chamber\", \"citations\"]\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Can you please search through all the ICTY trial judgments and appeal judgments and identify relevant paragraphs which would support the proposition that an individual who has previously joined enemy forces and is armed at the relevant point is considered to have lost their protected status at a particular point? Please determine whether the chamber undertakes a subjective or objective assessment?\",\n",
    "            \"expected_routing\": \"judgment\",\n",
    "            \"key_topics\": [\"enemy forces\", \"armed\", \"protected status\", \"subjective assessment\", \"objective assessment\", \"lost status\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"üß™ TESTING ENHANCED ICC RAG SYSTEM\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, query_info in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"LEGAL RESEARCH QUESTION {i}\")\n",
    "        print(f\"{'#'*80}\")\n",
    "        print(f\"Question: {query_info['question'][:150]}...\")\n",
    "        print(f\"Expected routing: {query_info['expected_routing']}\")\n",
    "        print(f\"Key topics: {', '.join(query_info['key_topics'])}\")\n",
    "        \n",
    "        # Retrieve context\n",
    "        context = rag_system.retrieve_context(query_info[\"question\"], top_k=8)\n",
    "        \n",
    "        # Generate legal analysis\n",
    "        analysis = rag_system.generate_legal_analysis(\n",
    "            query_info[\"question\"], \n",
    "            context, \n",
    "            conversation_id=f\"test_session_{i}\"\n",
    "        )\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nüìä ROUTING ANALYSIS:\")\n",
    "        print(f\"Expected: {query_info['expected_routing']}\")\n",
    "        print(f\"Actual: {context.routing_decision}\")\n",
    "        print(f\"Sources found: {context.total_sources}\")\n",
    "        print(f\"Processing time: {context.processing_time:.2f}s\")\n",
    "        \n",
    "        print(f\"\\n‚öñÔ∏è LEGAL ANALYSIS:\")\n",
    "        print(f\"Confidence score: {analysis.confidence_score:.3f}\")\n",
    "        print(f\"Key findings: {len(analysis.key_findings)}\")\n",
    "        print(f\"Citations: {len(analysis.citations)}\")\n",
    "        print(f\"Analysis length: {len(analysis.analysis)} characters\")\n",
    "        \n",
    "        print(f\"\\nüìù ANALYSIS PREVIEW:\")\n",
    "        print(analysis.analysis[:500] + \"...\" if len(analysis.analysis) > 500 else analysis.analysis)\n",
    "        \n",
    "        print(f\"\\nüîç KEY FINDINGS:\")\n",
    "        for j, finding in enumerate(analysis.key_findings[:3], 1):\n",
    "            print(f\"{j}. {finding}\")\n",
    "        \n",
    "        print(f\"\\nüìö CITATIONS:\")\n",
    "        for j, citation in enumerate(analysis.citations[:5], 1):\n",
    "            print(f\"{j}. {citation}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"question_id\": i,\n",
    "            \"question\": query_info[\"question\"],\n",
    "            \"routing_decision\": context.routing_decision,\n",
    "            \"sources_found\": context.total_sources,\n",
    "            \"confidence_score\": analysis.confidence_score,\n",
    "            \"analysis_length\": len(analysis.analysis),\n",
    "            \"key_findings_count\": len(analysis.key_findings),\n",
    "            \"citations_count\": len(analysis.citations),\n",
    "            \"processing_time\": context.processing_time + analysis.processing_time\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{'#'*80}\\n\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"üìä TEST SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    for result in results:\n",
    "        print(f\"Question {result['question_id']}: {result['routing_decision']} routing, \"\n",
    "              f\"{result['sources_found']} sources, {result['confidence_score']:.3f} confidence, \"\n",
    "              f\"{result['processing_time']:.2f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the test\n",
    "test_results = test_enhanced_rag_system()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da98526-6acf-46a5-9315-5dc42b99d3db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MLflow 3.0 Production Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eeb3fc2-6fe3-4387-950e-94a763947954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "class EnhancedICCRAGModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"MLflow 3.0 production model wrapper for Enhanced ICC RAG System.\"\"\"\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        \"\"\"Initialize the enhanced RAG system.\"\"\"\n",
    "        self.rag_system = EnhancedICCRAGSystem()\n",
    "    \n",
    "    def predict(self, context, model_input: pd.DataFrame) -> List[Dict]:\n",
    "        \"\"\"Handle predictions for serving endpoint.\"\"\"\n",
    "        try:\n",
    "            queries = model_input[\"query\"].tolist()\n",
    "            \n",
    "            # Extract optional parameters\n",
    "            num_results_list = model_input.get(\"num_results\", [8] * len(queries)).tolist()\n",
    "            conversation_ids = model_input.get(\"conversation_id\", [None] * len(queries)).tolist()\n",
    "            \n",
    "            results = []\n",
    "            for query, num_results, conv_id in zip(queries, num_results_list, conversation_ids):\n",
    "                try:\n",
    "                    # Retrieve context\n",
    "                    context = self.rag_system.retrieve_context(\n",
    "                        query=query,\n",
    "                        top_k=num_results if pd.notna(num_results) else 8\n",
    "                    )\n",
    "                    \n",
    "                    # Generate legal analysis\n",
    "                    analysis = self.rag_system.generate_legal_analysis(\n",
    "                        question=query,\n",
    "                        context=context,\n",
    "                        conversation_id=conv_id if pd.notna(conv_id) else None\n",
    "                    )\n",
    "                    \n",
    "                    # Format response\n",
    "                    result = {\n",
    "                        \"question\": query,\n",
    "                        \"analysis\": analysis.analysis,\n",
    "                        \"routing_decision\": context.routing_decision,\n",
    "                        \"sources_used\": len(analysis.sources_used),\n",
    "                        \"confidence_score\": analysis.confidence_score,\n",
    "                        \"key_findings\": analysis.key_findings,\n",
    "                        \"citations\": analysis.citations,\n",
    "                        \"processing_time_seconds\": context.processing_time + analysis.processing_time,\n",
    "                        \"conversation_id\": conv_id,\n",
    "                        \"sources\": [\n",
    "                            {\n",
    "                                \"source\": s.source,\n",
    "                                \"source_type\": s.source_type,\n",
    "                                \"section\": s.section,\n",
    "                                \"page_number\": s.page_number,\n",
    "                                \"article\": s.article,\n",
    "                                \"relevance_score\": round(s.score, 3)\n",
    "                            }\n",
    "                            for s in analysis.sources_used[:10]  # Top 10 sources\n",
    "                        ]\n",
    "                    }\n",
    "                    results.append(result)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # Handle individual query errors\n",
    "                    error_result = {\n",
    "                        \"question\": query,\n",
    "                        \"analysis\": f\"Error processing query: {str(e)}\",\n",
    "                        \"routing_decision\": \"error\",\n",
    "                        \"sources_used\": 0,\n",
    "                        \"confidence_score\": 0.0,\n",
    "                        \"key_findings\": [],\n",
    "                        \"citations\": [],\n",
    "                        \"processing_time_seconds\": 0,\n",
    "                        \"conversation_id\": conv_id,\n",
    "                        \"sources\": []\n",
    "                    }\n",
    "                    results.append(error_result)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            return [{\"error\": f\"Model error: {str(e)}\"}] * len(model_input)\n",
    "\n",
    "print(\"‚úÖ Enhanced ICC RAG Model for MLflow 3.0 defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57755189-e588-47e5-bc53-93ed02da4ee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Register the Enhanced ICC RAG Model in MLflow 3.0\n",
    "with mlflow.start_run(run_name=\"Enhanced_ICC_RAG_Production\") as run:\n",
    "    \n",
    "    # Create model instance\n",
    "    production_model = EnhancedICCRAGModel()\n",
    "    \n",
    "    # Input example for serving endpoint\n",
    "    input_example = pd.DataFrame({\n",
    "        \"query\": [\n",
    "            \"Can you please go through all the ICTY trial judgments and identify where the chamber discusses the status of an individual during the conflict?\",\n",
    "            \"What factors did the Trial Chamber rely on to assess active participation in hostilities?\"\n",
    "        ],\n",
    "        \"num_results\": [10, 12],\n",
    "        \"conversation_id\": [\"legal_research_001\", \"legal_research_001\"]\n",
    "    })\n",
    "    \n",
    "    # Expected output format\n",
    "    output_example = [\n",
    "        {\n",
    "            \"question\": \"Sample legal question\",\n",
    "            \"analysis\": \"Comprehensive legal analysis based on retrieved context...\",\n",
    "            \"routing_decision\": \"judgment\",\n",
    "            \"sources_used\": 8,\n",
    "            \"confidence_score\": 0.85,\n",
    "            \"key_findings\": [\"Key legal finding 1\", \"Key legal finding 2\"],\n",
    "            \"citations\": [\"Article 8\", \"Page 123\", \"Section A\"],\n",
    "            \"processing_time_seconds\": 5.2,\n",
    "            \"conversation_id\": \"legal_research_001\",\n",
    "            \"sources\": [\n",
    "                {\n",
    "                    \"source\": \"ICTY_Judgment_001.pdf\",\n",
    "                    \"source_type\": \"judgment\",\n",
    "                    \"section\": \"FINDINGS_OF_FACT\",\n",
    "                    \"page_number\": 123,\n",
    "                    \"article\": None,\n",
    "                    \"relevance_score\": 0.95\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Log the model using MLflow 3.0 syntax\n",
    "    mlflow.pyfunc.log_model(\n",
    "        name=\"icc_chatbot.search_model.enhanced_icc_rag_model\",\n",
    "        python_model=production_model,\n",
    "        input_example=input_example,\n",
    "        signature=infer_signature(input_example, output_example),\n",
    "        resources=[\n",
    "            DatabricksVectorSearchIndex(index_name=PAST_JUDGMENTS_INDEX),\n",
    "            DatabricksVectorSearchIndex(index_name=GENEVA_DOCUMENTATION_INDEX),\n",
    "            DatabricksServingEndpoint(endpoint_name=LLM_MODEL_ENDPOINT)\n",
    "        ],\n",
    "        pip_requirements=[\n",
    "            \"mlflow>=3.1.1\",\n",
    "            \"langchain\",\n",
    "            \"databricks-langchain\",\n",
    "            \"numpy\",\n",
    "            \"pandas\",\n",
    "            \"pydantic\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Register model in Unity Catalog\n",
    "    model_uri = f\"runs:/{run.info.run_id}/enhanced_icc_rag_model\"\n",
    "    registered_model = mlflow.register_model(\n",
    "        model_uri=model_uri,\n",
    "        name=\"icc_chatbot.search_model.enhanced_icc_rag_legal_research\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model logged: {run.info.run_id}\")\n",
    "    print(f\"üîó Model URI: {model_uri}\")\n",
    "    print(f\"üì¶ Model registered: {registered_model.name} v{registered_model.version}\")\n",
    "    print(f\"üåê View in Unity Catalog: https://dbc-0619d7f5-0bda.cloud.databricks.com/explore/data/models/{registered_model.name}/version/{registered_model.version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2264d1ec-0c8d-47b7-bcc9-5c3d9c12956c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Usage Examples & Deployment Instructions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a736d74-e8ba-44cf-8bca-1d1eb7be3830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_usage_examples():\n",
    "    \"\"\"Show comprehensive usage examples for the Enhanced ICC RAG System.\"\"\"\n",
    "    \n",
    "    print(\"üöÄ ENHANCED ICC RAG SYSTEM - USAGE EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nüìã 1. LOCAL USAGE:\")\n",
    "    print(\"\"\"\n",
    "# Initialize the system\n",
    "rag_system = EnhancedICCRAGSystem()\n",
    "\n",
    "# Simple legal research query\n",
    "question = \"What are the elements of crimes against humanity?\"\n",
    "context = rag_system.retrieve_context(question, top_k=8)\n",
    "analysis = rag_system.generate_legal_analysis(question, context)\n",
    "\n",
    "print(f\"Analysis: {analysis.analysis}\")\n",
    "print(f\"Confidence: {analysis.confidence_score}\")\n",
    "print(f\"Sources: {len(analysis.sources_used)}\")\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"\\nüìã 2. CONVERSATIONAL USAGE:\")\n",
    "    print(\"\"\"\n",
    "# Multi-turn conversation\n",
    "conversation_id = \"legal_research_session_001\"\n",
    "\n",
    "# First question\n",
    "question1 = \"How has the principle of proportionality been applied in ICTY judgments?\"\n",
    "context1 = rag_system.retrieve_context(question1, top_k=10)\n",
    "analysis1 = rag_system.generate_legal_analysis(question1, context1, conversation_id)\n",
    "\n",
    "# Follow-up question (with memory)\n",
    "question2 = \"What factors did the chamber consider in those cases?\"\n",
    "context2 = rag_system.retrieve_context(question2, top_k=8)\n",
    "analysis2 = rag_system.generate_legal_analysis(question2, context2, conversation_id)\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"\\nüìã 3. SERVING ENDPOINT USAGE:\")\n",
    "    print(\"\"\"\n",
    "# Deploy to serving endpoint\n",
    "import requests\n",
    "\n",
    "endpoint_url = \"https://your-workspace.cloud.databricks.com/serving-endpoints/enhanced-icc-rag/invocations\"\n",
    "headers = {\"Authorization\": \"Bearer YOUR_TOKEN\"}\n",
    "\n",
    "# Single query\n",
    "payload = {\n",
    "    \"dataframe_split\": {\n",
    "        \"columns\": [\"query\", \"num_results\", \"conversation_id\"],\n",
    "        \"data\": [[\"What are the requirements for combatant status?\", 10, \"session_001\"]]\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(endpoint_url, headers=headers, json=payload)\n",
    "result = response.json()[\"predictions\"][0]\n",
    "\n",
    "print(f\"Analysis: {result['analysis']}\")\n",
    "print(f\"Routing: {result['routing_decision']}\")\n",
    "print(f\"Sources: {result['sources_used']}\")\n",
    "print(f\"Confidence: {result['confidence_score']}\")\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"\\nüìã 4. BATCH PROCESSING:\")\n",
    "    print(\"\"\"\n",
    "# Multiple legal research questions\n",
    "batch_payload = {\n",
    "    \"dataframe_split\": {\n",
    "        \"columns\": [\"query\", \"num_results\", \"conversation_id\"],\n",
    "        \"data\": [\n",
    "            [\"Can you identify all ICTY judgments discussing civilian status?\", 12, \"batch_001\"],\n",
    "            [\"What factors determine active participation in hostilities?\", 10, \"batch_001\"],\n",
    "            [\"How do chambers assess subjective vs objective criteria?\", 8, \"batch_001\"]\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(endpoint_url, headers=headers, json=batch_payload)\n",
    "results = response.json()[\"predictions\"]\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"Question {i}: {result['routing_decision']} routing, {result['sources_used']} sources\")\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"\\nüìã 5. DEPLOYMENT INSTRUCTIONS:\")\n",
    "    print(\"\"\"\n",
    "# Step 1: Create serving endpoint\n",
    "# Go to Databricks UI > Serving > Create Endpoint\n",
    "# Select the registered model: enhanced_icc_rag_legal_research\n",
    "# Configure compute and scaling\n",
    "\n",
    "# Step 2: Test endpoint\n",
    "# Use the test queries provided above\n",
    "# Monitor performance and adjust scaling as needed\n",
    "\n",
    "# Step 3: Integration\n",
    "# Integrate with your legal research workflow\n",
    "# Use conversation_id for multi-turn research sessions\n",
    "# Monitor confidence scores for quality assurance\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"\\nüìã 6. OPTIMAL CONFIGURATION:\")\n",
    "    print(\"\"\"\n",
    "# Query types and recommended num_results:\n",
    "# - Complex legal research: 10-15\n",
    "# - Specific case law queries: 8-12  \n",
    "# - Geneva Convention queries: 6-10\n",
    "# - Factual questions: 4-8\n",
    "\n",
    "# Routing decisions:\n",
    "# - \"judgment\": ICTY/ICC case law queries\n",
    "# - \"geneva\": International humanitarian law queries  \n",
    "# - \"both\": Comparative legal analysis\n",
    "\n",
    "# Confidence scores:\n",
    "# - >0.8: High confidence, reliable analysis\n",
    "# - 0.6-0.8: Good confidence, review recommended\n",
    "# - <0.6: Low confidence, additional research needed\n",
    "\"\"\")\n",
    "\n",
    "# Show usage examples\n",
    "show_usage_examples()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ICC_Enhanced_RAG_Production",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
